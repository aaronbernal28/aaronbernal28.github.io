---
title: "Estaditica Bayesiana"
author: "Aaron Bernal Huanca"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

## Temas

-   Estadística Bayesiana (hoy)
-   Modelos lineal generalizados\
-   Análisis de supervivencia\
-   Modelos lineales mixtos\
-   Regresión con splines\
-   Causalidad

------------------------------------------------------------------------

## Teorica: Estadística Bayesiana

*(Aquí iría un diagrama de caja ilustrando la dependencia entre la variable aleatoria θ y las observaciones X.)*

| Estado | Verosimilitud $\ell(\text{estado}\mid \theta)$ | Prior $p(\theta)$ |
|:-------------------|-------------------------------:|-------------------:|
| LA     |                                          $0.8$ |             $0.6$ |
| LHASA  |                                          $0.2$ |     $0.001\ldots$ |
| GATO   |                                         $0.01$ |            $0.01$ |

### Definición

Sean la muestra aleatoria $$ X_1, \dots, X_n \overset{\text{iid}}{\sim} p_\theta, $$ tal que $p_\theta \in \mathcal{P} = \{p_\omega: \omega \in \Omega \}$.

-   **Estadística frecuentista**:\
    $\theta$ es un valor desconocido y determinístico.

-   **Estadística bayesiana**:\
    $\theta$ es una realización de la variable aleatoria $\Theta \sim \Lambda$, y además $$
    X_i|_{\Theta = \theta} \sim p_\theta$$

### Estimación Bayesiana

Disponemos de dos fuentes de informacion para estimar $\theta$:

1.  **Priori** $\Theta$.\
2.  **Datos** $X = (X_1,\dots,X_n)$.

Buscamos un estimador $\delta(X)$ que minimice la pérdida cuadrática (riesgo de Bayes) $$
r(\delta, \Theta) \\
= \mathbb{E} [(\delta(X)-\Theta )^2] \\
= \mathbb{E}_\Theta [ \mathbb{E}_{X}[(\delta(X) - \Theta )^2 | \Theta = \theta]] \\
= \mathbb{E}_{X}[\ \mathbb{E}_{\Theta}[(\delta(X)-\theta)^2 | X=x]].
$$

Minimizando punto a punto, $$
\delta^*(x) =\arg\min_\delta \mathbb{E}_\Theta [(\delta(X)-\Theta)^2 | X=x] \\
$$ desarrollando la esperanza condicional $$
\mathbb{E}_\Theta [(\delta(X)-\Theta)^2 | X=x] \\
= \mathbb{E}_\Theta [\delta^2(X) | X=x] 
  + \mathbb{E}_\Theta [-2 \delta(X) \times \Theta | X=x]
  + \mathbb{E}_\Theta [ \Theta^2| X=x] \\
= \delta^2(x)
  - 2 \times \delta(x) \times \mathbb{E}[\Theta | X=x]
  + \mathbb{E} [ \Theta^2| X=x] \\
$$ Luego, $$
\delta^*(x) =\arg\min_\delta \{\delta^2(x) - 2 \times \delta(x) \times \mathbb{E}[\Theta | X=x]\} \\
\Rightarrow \delta^*(x) = \mathbb{E}[\Theta | X=x] \\
$$ El estimador de Bayes es $$ 
\hat{\delta}(X) = \mathbb{E}[\Theta | X] $$

## Estimación Bayesiana: Ejemplo Beta–Bernoulli

Supongamos $\Theta \sim \mathrm{Beta} (a, b)$ y $X_i|_{\Theta = \theta} \sim \mathrm{Ber}(\theta)$

-   Priori $$
      f_T(\theta)= \mathrm{Beta}(a,b)
      = \frac{\Gamma(a+b)}{\Gamma(a)\,\Gamma(b)} \theta^{a-1}(1-\theta)^{b-1} \mathbf{1}_{(0,1)}(\theta),
    $$

con $\mathbb{E}[\theta]=\frac{a}{a+b}.$

### Posterior

Por Bayes, $$
f(\Theta = \theta \mid X = x) \\
= \frac{f(X = x \mid \Theta = \theta) \times f(\Theta = \theta)}{f(X = x)} \\
\propto f(X = x \mid \Theta = \theta) \times f(\Theta = \theta) \\
\propto \prod_{i=1}^n \theta^{x_i} (1 - \theta)^{1 - x_i} \times 
\frac{\Gamma(a+b)}{\Gamma(a)\,\Gamma(b)} \theta^{a-1}(1-\theta)^{b-1} \mathbf{1}_{(0,1)}(\theta)\\
\propto \theta^{\sum_{i=1}^n x_i + a - 1} (1-\theta)^{n - \sum_{i=1}^n x_i + b - 1} \mathbf{1}_{(0,1)}(\theta)
$$ luego $$
  \Theta |_{X = x}\sim
  \mathrm{Beta}\bigl(\sum_{i=1}^n x_i + a, n-\sum_{i=1}^n x_i + b)
$$ Donde $n$ es el tamaño de la muestra y $x_i$ es la i-ésima observación

### Estimador Bayesiano (pérdida cuadrática) calculando la esperanza

$$
\hat{\delta}(x)
= \mathbb{E}[\Theta\mid X = x]
= \frac{\sum_{i=1}^m x_i + a}{n + a + b}
$$

Comparación con el estimador frecuentista: $$
\hat\theta_{\mathrm{frequentista}}
= \frac{\sum_i x_i}{m} \\
\hat\delta(x)
= \frac{n}{n+a+b}\,\frac{\sum_i x_i}{n} + \frac{a+b}{n+a+b}\ \frac{a}{a+b}.
$$

## Ejercicio 1

Sea $\mathbf{X} = (X_1, \ldots, X_n)$ una muestra aleatoria con distribución $P_\theta$. Suponga una distribución a priori $T$ para $\theta$, con $T \sim \tau$. Sea $\delta_\Lambda(\mathbf{X})$ el estimador Bayes para $\theta$. Si es insesgado, pruebe que $$
\mathbb{E}((\delta_\Lambda(\mathbf{X}) - T)^2) = 0.
$$

Como $\delta(X)$ es estimador de Bayes\
$$
\delta(X) = \mathbb{E}(\Theta \mid X)
$$\
Como es insesgado, usando esperanzas iteradas\
$$
\Rightarrow \mathbb{E}(\delta(X)|\Theta = \theta) = \theta \text{ } \forall \theta \\
\Rightarrow \mathbb{E}(\delta(X)) = \mathbb{E}(\Theta)
\Rightarrow \delta(X) = \mathbb{E}(\mathbb{E}(\Theta \mid X)) = \mathbb{E}(\Theta)
$$

$$
\begin{aligned}
\mathbb{E}((\delta(X) - \Theta)^2) 
&= \mathbb{E}\left( \left( \delta(X) - \mathbb{E}(\delta(X) \mid \Theta) \right)^2 \right) \\
&= \mathbb{E}\left( \left( \delta(X) - \mathbb{E}(\delta(X)) \right)^2 \right) \\
&= \mathbb{E} \left( \operatorname{Var}( \delta(X) \mid \Theta ) \right) \\
&= \operatorname{Var}(\delta(X)) - \operatorname{Var}(\mathbb{E}(\delta(X) \mid \Theta)) \\
&= \operatorname{Var}(\delta(X)) - \operatorname{Var}(\Theta) \\
&= \mathbb{E}(\delta^2(X)) - \mathbb{E}(\Theta^2)
\end{aligned}
$$

Basta ver que

$$
\begin{aligned}
\mathbb{E}(\delta(X)\Theta) 
&= \mathbb{E}( \mathbb{E}(\delta(X)\Theta \mid \Theta) ) \\
&= \mathbb{E}( \Theta \mathbb{E}(\delta(X) \mid \Theta) ) \\
&= \mathbb{E}(\Theta^2)
\end{aligned}
$$

$$
\begin{aligned}
\mathbb{E}(\delta(X)\Theta) 
&= \mathbb{E}( \mathbb{E}(\delta(X)\Theta \mid X) ) \\
&= \mathbb{E}( \delta(X) \mathbb{E}(\Theta \mid X) ) \\
&= \mathbb{E}(\delta^2(X))
\end{aligned}
$$

Luego $\mathbb{E}((\delta(X)-\Theta)^2)=0$ como se queria ver.
